{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: 2-Columns from crawler\n",
    "#Output: 7-Columns preprocessed data to elasticsearch and gaam5 and cassandra\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "#if this code is running for first time, it needs to train the model using old data.\n",
    "FIRST_TIME=True #ino bezarin True bemoone. bazi computera nemitoone modele spark ro load kone. buge sparke. model to train konin tori nist.\n",
    "pathModel = r\"C:\\Users\\AhmaDGoly\\Desktop\\Arshad\\TERM 2\\Big Data\\Final\\Codes\\ScoreModel.pysparkmodel\\\\\"   #ye addrese pooshe bedin ke khali bashe.\n",
    "pathOldData6Column = r\"C:\\Users\\AhmaDGoly\\Desktop\\Arshad\\TERM 2\\Big Data\\Final\\Codes\\6ColumnOldData.csv\" #inja save mishe. ye esme alaki bedin mohem nis.\n",
    "pathOldData = r'C:\\Users\\AhmaDGoly\\Desktop\\Arshad\\TERM 2\\Big Data\\Final\\Codes\\OldData.csv'               #in mohem hast. addresse OldData.csv ke ferestadamo bedin.\n",
    "topicToReceive = 'crawler'\n",
    "topicToSend1 = \"elasticsearch\"\n",
    "topicToSend2 = \"gaam5\"\n",
    "topicToSend3 = \"cassandra\"\n",
    "kafkaPort = 19092\n",
    "\n",
    "def current_time():\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    return current_datetime.strftime(\"[%Y-%m-%d_%H:%M:%S]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persian_num_to_eng_num(number):\n",
    "    return number.replace('۰', '0').replace('۱', '1')\\\n",
    "                                   .replace('۲', '2')\\\n",
    "                                   .replace('۳', '3')\\\n",
    "                                   .replace('۴', '4')\\\n",
    "                                   .replace('۵', '5')\\\n",
    "                                   .replace('۶', '6')\\\n",
    "                                   .replace('۷', '7')\\\n",
    "                                   .replace('۸', '8')\\\n",
    "                                   .replace('۹', '9')\n",
    "\n",
    "\n",
    "def extract_hashtags(message):\n",
    "    if pd.isna(message):\n",
    "        return []\n",
    "    hashtags = re.findall(r'\\#\\w+', message)\n",
    "    modified_hashtags = []\n",
    "    for hashtag in hashtags:\n",
    "        if hashtag.startswith('#ب') and hashtag.endswith('ن'):\n",
    "            modified_hashtags.append('#BTC')\n",
    "        elif hashtag.startswith('#اتر'):\n",
    "            modified_hashtags.append('#ETH')\n",
    "        elif hashtag.startswith('#بیت'):\n",
    "            modified_hashtags.append('#BTC')\n",
    "        else:\n",
    "            modified_hashtags.append(hashtag)\n",
    "    return modified_hashtags\n",
    "\n",
    "\n",
    "def assign_score(message):\n",
    "    if 'فوری' in message and 'فاندای منفی' in message:\n",
    "        return 10\n",
    "    elif 'فاندای منفی' in message:\n",
    "        return 30\n",
    "    elif 'فوری' in message and 'فاندای مثبت' in message:\n",
    "        return 90\n",
    "    elif 'فاندای مثبت' in message:\n",
    "        return 70\n",
    "    if '#لیکوئید' in message and '۲۴' in message:\n",
    "        if 'Long' in message:\n",
    "            percent_index = message.find('٪')\n",
    "            if percent_index != -1:\n",
    "                start_index = message.rfind(' ', 0, percent_index) + 1\n",
    "                percentage = message[start_index:percent_index]\n",
    "                return 100 - int(round(float(persian_num_to_eng_num(percentage))))\n",
    "        elif 'Short' in message:\n",
    "            percent_index = message.find('٪')\n",
    "            if percent_index != -1:\n",
    "                start_index = message.rfind(' ', 0, percent_index) + 1\n",
    "                percentage = message[start_index:percent_index]\n",
    "                return int(round(float(persian_num_to_eng_num(percentage))))\n",
    "    return 50\n",
    "\n",
    "def assign_category(message):\n",
    "    if message.startswith('📊'):\n",
    "        return 'Chart'\n",
    "    elif message.startswith('✍') or message.startswith('📉') or message.startswith('🔰'):\n",
    "        return 'News'\n",
    "    elif message.startswith('🚨 فوری'):\n",
    "        return 'Immediate News'\n",
    "    elif message.startswith('🚨'):\n",
    "        return 'Special News'\n",
    "    else:\n",
    "        return 'News'\n",
    "def clean_message_text(message):\n",
    "    message = message.replace('\\n', '')\n",
    "    message = message.replace('@NEWS_FUNDA', '')\n",
    "    message = message.replace('NEWS_FUNDA', '')\n",
    "    message = message.strip()\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoColumn_to_SixColumn(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['hashtags'] = df_copy['message'].apply(extract_hashtags)\n",
    "    df_copy = df_copy.dropna(subset=['message'])\n",
    "    df_copy['score'] = df_copy['message'].apply(assign_score)\n",
    "    df_copy['category'] = df_copy['message'].apply(assign_category)\n",
    "    df_copy['clean_message'] = df_copy['message'].apply(clean_message_text)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "spark = SparkSession.builder.appName(\"MessageScorePrediction\").getOrCreate()\n",
    "\n",
    "def pysparkScoreModel(df,spark):\n",
    "    print(f\"{current_time()}: Training ScoreModel based on old dataset.\")\n",
    "    data = [(row.clean_message, row.score) for row in df.itertuples(index=False)]\n",
    "    spark_df = spark.createDataFrame(data, [\"clean_message\", \"score\"])\n",
    "    tokenizer = Tokenizer(inputCol=\"clean_message\", outputCol=\"words\")\n",
    "    tokenized_df = tokenizer.transform(spark_df)\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "    featured_df = hashingTF.transform(tokenized_df)\n",
    "    (training_data, testing_data) = featured_df.randomSplit([0.9, 0.1], seed=42)\n",
    "    lr = LinearRegression(labelCol=\"score\", featuresCol=\"features\")\n",
    "    model = lr.fit(training_data)\n",
    "    #Test:\n",
    "    predictions = model.transform(testing_data)\n",
    "    evaluator = RegressionEvaluator(labelCol=\"score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"{current_time()}: Training ScoreModel completed. Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    #return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pandas_df, spark, scoreModel):\n",
    "    #add some prints mb?\n",
    "    data = [(row.clean_message,) for row in pandas_df.itertuples(index=False)]\n",
    "    schema = StructType([StructField(\"clean_message\", StringType())])\n",
    "    spark_df = spark.createDataFrame(data, schema)\n",
    "    tokenizer = Tokenizer(inputCol=\"clean_message\", outputCol=\"words\")\n",
    "    tokenized_df = tokenizer.transform(spark_df)\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "    featured_df = hashingTF.transform(tokenized_df)\n",
    "    predictions = scoreModel.transform(featured_df)\n",
    "    predictions = predictions.drop(\"features\")\n",
    "    predictions = predictions.drop(\"words\")\n",
    "    predictions = predictions.drop(\"clean_message\")\n",
    "    predictions = predictions.withColumnRenamed(\"prediction\", \"ML_score\")\n",
    "    pandas_predictions = predictions.toPandas()\n",
    "    return pandas_df.merge(pandas_predictions, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-15_16:34:50]: FIRST_TIME is on. loading old data.\n",
      "[2023-07-15_16:34:51]: Preprocessed old data. made 'hashtag','score','category' and 'clean_message' columns from message column.\n",
      "[2023-07-15_16:34:51]: Training ScoreModel based on old dataset.\n",
      "[2023-07-15_16:35:37]: Training ScoreModel completed. Root Mean Squared Error (RMSE): 3.865693387791679\n",
      "[2023-07-15_16:35:39]: Saved Score Model and 6Column old_data.\n"
     ]
    }
   ],
   "source": [
    "if FIRST_TIME:\n",
    "    print(f\"{current_time()}: FIRST_TIME is on. loading old data.\")\n",
    "    df = pd.read_csv(pathOldData, encoding='utf-8-sig')\n",
    "    df = TwoColumn_to_SixColumn(df)\n",
    "    print(f\"{current_time()}: Preprocessed old data. made 'hashtag','score','category' and 'clean_message' columns from message column.\")\n",
    "    scoreModel = pysparkScoreModel(df,spark)\n",
    "    shutil.rmtree(pathModel, ignore_errors=True)\n",
    "    scoreModel.save(pathModel)\n",
    "    df.to_csv(pathOldData6Column, index=False, encoding='utf-8-sig')\n",
    "    print(f\"{current_time()}: Saved Score Model and 6Column old_data.\")\n",
    "else:\n",
    "    print(f\"{current_time()}: FIRST_TIME is off. loading ScoreModel.\")\n",
    "    scoreModel = LinearRegressionModel.load(pathModel)\n",
    "    df = pd.read_csv(pathOldData6Column, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerenating Preprocessed Stream_data for group.\n",
    "#streamdf = pd.read_csv(r'C:\\Users\\AhmaDGoly\\Desktop\\Arshad\\TERM 2\\Big Data\\Final\\Codes\\StreamData.csv', encoding='utf-8-sig')\n",
    "#streamdf = TwoColumn_to_SixColumn(streamdf)\n",
    "#streamdf = predict(streamdf, spark, scoreModel)\n",
    "#streamdf.to_csv(r'C:\\Users\\AhmaDGoly\\Desktop\\Arshad\\TERM 2\\Big Data\\Final\\Codes\\StreamData_preprocessed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer,KafkaProducer\n",
    "consumer = KafkaConsumer(topicToReceive, bootstrap_servers=[f'127.0.0.1:{kafkaPort}'])\n",
    "producer = KafkaProducer(bootstrap_servers=[f'127.0.0.1:{kafkaPort}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-13_18:56:20]: 1: preProcessingThread is running.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "receivedMsgs_queue = queue.Queue()\n",
    "ThreadOutput_queue = queue.Queue()\n",
    "x = 1\n",
    "y = 1\n",
    "def TwoColumn_To_SevenColumn_Thread():\n",
    "    global receivedMsgs_queue\n",
    "    global ThreadOutput_queue\n",
    "    global producer\n",
    "    global x\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "        if not receivedMsgs_queue.empty():\n",
    "            message = receivedMsgs_queue.get()\n",
    "            json_row = message.value.decode('utf-8')\n",
    "            row = json.loads(json_row)\n",
    "            cachedf = pd.DataFrame([row])\n",
    "            #2 to 6\n",
    "            cachedf = TwoColumn_to_SixColumn(cachedf)\n",
    "            #6 to 7 (ML_score from ScoreModel)\n",
    "            cachedf = predict(cachedf, spark, scoreModel)\n",
    "            #print(f\"{current_time()}: Processing complete. sending preprocessed news to topic={topicToSend1},{topicToSend2},{topicToSend3}\")\n",
    "            ThreadOutput_queue.put(f\"{current_time()}: Processing {x} complete. sending preprocessed news to topic={topicToSend1},{topicToSend2},{topicToSend3}\")\n",
    "            for index, row in cachedf.iterrows():\n",
    "                json_row = json.dumps(row.to_dict())\n",
    "                producer.send(topicToSend1, value=json_row.encode('utf-8'))\n",
    "                producer.send(topicToSend2, value=json_row.encode('utf-8'))\n",
    "                producer.send(topicToSend3, value=json_row.encode('utf-8'))\n",
    "            x = x + 1\n",
    "            del cachedf,row,json_row\n",
    "\n",
    "print(f\"{current_time()}: {x}: preProcessingThread is running.\")\n",
    "preProcessingThread = threading.Thread(target=TwoColumn_To_SevenColumn_Thread)\n",
    "preProcessingThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-13_18:56:38]: 1: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:57:08]: 2: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:56:45]: Processing 1 complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-13_18:57:38]: 3: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:57:14]: Processing 2 complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-13_18:58:08]: 4: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:57:43]: Processing 3 complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-13_18:58:47]: 5: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:13]: Processing 4 complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-13_18:58:47]: 6: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 7: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 8: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 9: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 10: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 11: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 12: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 13: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 14: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 15: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 16: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 17: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 18: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 19: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 20: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 21: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 22: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 23: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 24: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 25: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 26: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 27: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 28: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 29: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 30: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 31: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 32: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 33: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 34: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 35: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 36: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 37: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 38: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 39: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 40: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 41: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 42: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 43: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 44: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 45: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 46: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 47: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 48: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 49: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 50: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 51: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 52: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 53: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 54: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 55: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 56: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 57: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 58: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 59: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 60: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 61: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 62: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 63: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 64: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 65: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 66: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 67: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 68: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 69: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 70: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 71: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 72: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 73: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 74: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 75: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 76: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 77: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 78: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 79: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 80: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 81: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 82: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 83: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 84: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 85: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 86: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 87: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 88: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 89: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 90: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 91: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 92: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 93: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 94: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 95: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 96: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 97: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 98: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 99: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 100: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 101: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 102: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 103: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 104: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 105: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 106: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 107: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 108: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 109: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 110: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 111: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 112: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 113: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 114: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 115: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 116: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 117: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 118: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 119: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 120: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 121: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 122: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 123: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 124: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 125: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 126: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 127: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 128: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 129: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 130: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 131: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 132: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 133: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 134: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 135: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 136: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 137: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 138: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 139: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 140: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 141: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 142: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 143: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 144: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 145: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 146: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 147: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 148: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 149: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 150: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 151: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 152: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 153: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 154: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 155: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 156: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 157: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 158: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 159: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 160: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 161: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 162: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 163: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 164: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 165: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 166: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 167: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 168: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 169: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 170: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 171: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 172: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 173: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 174: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 175: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 176: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 177: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 178: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 179: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 180: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 181: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 182: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 183: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 184: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 185: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 186: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 187: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 188: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 189: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 190: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 191: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 192: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 193: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 194: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 195: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 196: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 197: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 198: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 199: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 200: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 201: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 202: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 203: Received a news from Crawler. Processing...\n",
      "[2023-07-13_18:58:47]: 204: Received a news from Crawler. Processing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m consumer:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcurrent_time()\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m: Received a news from Crawler. Processing...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     y \u001b[39m=\u001b[39m y \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[0;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[0;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[1;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[0;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[0;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[0;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[0;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[1;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[0;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[0;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[0;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[0;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[0;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\selectors.py:324\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     r, w, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_readers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_writers, [], timeout)\n\u001b[0;32m    325\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\selectors.py:315\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_select\u001b[39m(\u001b[39mself\u001b[39m, r, w, _, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 315\u001b[0m     r, w, x \u001b[39m=\u001b[39m select\u001b[39m.\u001b[39;49mselect(r, w, w, timeout)\n\u001b[0;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m r, w \u001b[39m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for message in consumer:\n",
    "    print(f\"{current_time()}: {y}: Received a news from Crawler. Processing...\")\n",
    "    y = y + 1\n",
    "    receivedMsgs_queue.put(message)\n",
    "    while not ThreadOutput_queue.empty():\n",
    "        print(ThreadOutput_queue.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-11_20:46:39]: 1: Received a news from Crawler. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.cluster:Topic elasticsearch is not available during auto-create initialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-11_20:46:44]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.cluster:Topic elasticsearch is not available during auto-create initialization\n",
      "WARNING:kafka.cluster:Topic gaam5 is not available during auto-create initialization\n",
      "WARNING:kafka.cluster:Topic gaam5 is not available during auto-create initialization\n",
      "WARNING:kafka.cluster:Topic gaam5 is not available during auto-create initialization\n",
      "WARNING:kafka.cluster:Topic cassandra is not available during auto-create initialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-11_20:46:46]: 2: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:46:51]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:46:51]: 3: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:46:56]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:46:56]: 4: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:47:01]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:47:01]: 5: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:47:06]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:47:06]: 6: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:47:10]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:47:10]: 7: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:47:15]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:47:15]: 8: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:47:21]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:47:21]: 9: Received a news from Crawler. Processing...\n",
      "[2023-07-11_20:47:27]: Processing complete. sending preprocessed news to topic=elasticsearch,gaam5,cassandra\n",
      "[2023-07-11_20:47:27]: 10: Received a news from Crawler. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"c:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"c:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m cachedf \u001b[39m=\u001b[39m TwoColumn_to_SixColumn(cachedf)\n\u001b[0;32m      9\u001b[0m \u001b[39m#6 to 7 (ML_score from ScoreModel)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m cachedf \u001b[39m=\u001b[39m predict(cachedf, spark, scoreModel)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcurrent_time()\u001b[39m}\u001b[39;00m\u001b[39m: Processing complete. sending preprocessed news to topic=\u001b[39m\u001b[39m{\u001b[39;00mtopicToSend1\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mtopicToSend2\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mtopicToSend3\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m cachedf\u001b[39m.\u001b[39miterrows():\n",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(pandas_df, spark, scoreModel)\u001b[0m\n\u001b[0;32m     11\u001b[0m predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mwithColumnRenamed(\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mML_score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m pandas_predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39;49mtoPandas()\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_predictions\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m pdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_records(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(), columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    206\u001b[0m column_counter \u001b[39m=\u001b[39m Counter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m    208\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \n\u001b[0;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[1;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[0;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Old output without using thread and queue\n",
    "\n",
    "#x = 1\n",
    "# let this cell run forever.\n",
    "#for message in consumer:\n",
    "#    print(f\"{current_time()}: {x}: Received a news from Crawler. Processing...\")\n",
    "#    json_row = message.value.decode('utf-8')\n",
    "#    row = json.loads(json_row)\n",
    "#    cachedf = pd.DataFrame([row])\n",
    "#    #2 to 6\n",
    "#    cachedf = TwoColumn_to_SixColumn(cachedf)\n",
    "#    #6 to 7 (ML_score from ScoreModel)\n",
    "#    cachedf = predict(cachedf, spark, scoreModel)\n",
    "#    print(f\"{current_time()}: Processing complete. sending preprocessed news to topic={topicToSend1},{topicToSend2},{topicToSend3}\")\n",
    "#    for index, row in cachedf.iterrows():\n",
    "#        json_row = json.dumps(row.to_dict())\n",
    "#        producer.send(topicToSend1, value=json_row.encode('utf-8'))\n",
    "#        producer.send(topicToSend2, value=json_row.encode('utf-8'))\n",
    "#        producer.send(topicToSend3, value=json_row.encode('utf-8'))\n",
    "#    x = x + 1\n",
    "#    del cachedf,row,json_row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfe9f1fea70b6f3c892a507b25933db96548476c663a42d0d97c2442f6e3001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
