{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age in code ro nafahmidin khodetoon ye code joda ba estefade az StreamData_preprocessed.csv benevisin. yani\n",
    "#ye code benevisin ke StreamData_preprocessed.csv ro load kone va satr hasho yeki yeki pass bede be codetoon.\n",
    "#input: 7-column-rows\n",
    "#output: just save it to cassandra. and write an app to get a hashtag as input and show the articles about that hashtag in python (show the output in console).\n",
    "import json,datetime\n",
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "topicToReceive = 'cassandra'\n",
    "kafkaPort = 19092\n",
    "cassandraPort = 9042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to cassanda.\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import SimpleStatement\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Cassandra cluster\n",
    "cluster = Cluster(['127.0.0.1'], port=cassandraPort)\n",
    "session = cluster.connect()\n",
    "print('Connected to cassanda.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyspace = 'telegram_news'\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS telegram_news WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\")\n",
    "session.set_keyspace(keyspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x2648488adf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS telegram_news.hashtag (\n",
    "        message text,\n",
    "        hashtag text,\n",
    "        PRIMARY KEY (message, hashtag)\n",
    "    )\n",
    "\"\"\"\n",
    "session.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x26484890760>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS telegram_news.date (\n",
    "        date timestamp,\n",
    "        year int,\n",
    "        month int,\n",
    "        day int,\n",
    "        hour int,\n",
    "        minute int,\n",
    "        message text,\n",
    "        PRIMARY KEY (date, year, month, day, hour)\n",
    "    )\n",
    "\"\"\"\n",
    "session.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    return current_datetime.strftime(\"[%Y-%m-%d_%H:%M:%S]\")\n",
    "\n",
    "consumer = KafkaConsumer(topicToReceive, bootstrap_servers=[f'127.0.0.1:{kafkaPort}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-17_18:35:02]: MsgQueue is running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: 'Queue' object is not callable\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "MsgQueue = queue.Queue()\n",
    "def QueueProcessor():\n",
    "    global MsgQueue\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "        if not MsgQueue.empty():\n",
    "            row = MsgQueue.get()\n",
    "            # 1\n",
    "            date_str = row['date']\n",
    "            date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "            year = date.year\n",
    "            month = date.month\n",
    "            day = date.day\n",
    "            hour = date.hour\n",
    "            minute = date.minute\n",
    "            message = row['message']\n",
    "            insert_query = \"INSERT INTO telegram_news.date (date, year, month, day, hour, minute, message) VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "            session.execute(insert_query, (date, year, month, day, hour, minute, message))\n",
    "            # 2\n",
    "            hashtags = row['hashtags']\n",
    "            hashtags = ast.literal_eval(hashtags)\n",
    "            if len(hashtags) == 0:\n",
    "                continue\n",
    "            # If there is a single item, add it as a record in the table\n",
    "            if len(hashtags) == 1:\n",
    "                hashtag = hashtags[0]\n",
    "                insert_query = \"INSERT INTO telegram_news.hashtag (message, hashtags) VALUES (%s, %s)\"\n",
    "                session.execute(insert_query, (message, hashtag))\n",
    "            else:\n",
    "                # If there are multiple items, add each one separately as separate records\n",
    "                for hashtag in hashtags:\n",
    "                    insert_query = \"INSERT INTO telegram_news.hashtag (message, hashtags) VALUES (%s, %s)\"\n",
    "                    session.execute(insert_query, (message, hashtag))\n",
    "\n",
    "MsgQueueThread = threading.Thread(target=MsgQueue)\n",
    "MsgQueueThread.start()\n",
    "print(f\"{current_time()}: MsgQueue is running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-17_18:36:40]: 1: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:37:06]: 2: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:37:33]: 3: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:38:02]: 4: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:38:34]: 5: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:39:02]: 6: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:39:33]: 7: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:40:05]: 8: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:40:35]: 9: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:41:05]: 10: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:41:32]: 11: Received a news from Preprocessing_Thread. putting it to cassandra...\n",
      "[2023-07-17_18:42:04]: 12: Received a news from Preprocessing_Thread. putting it to cassandra...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m consumer:\n\u001b[0;32m      3\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcurrent_time()\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m: Received a news from Preprocessing_Thread. putting it to cassandra...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_v1()\n\u001b[0;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_v2()\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[0;32m   1202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_message_generator_v2\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1115\u001b[0m     timeout_ms \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consumer_timeout \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime())\n\u001b[1;32m-> 1116\u001b[0m     record_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms, update_offsets\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m   1117\u001b[0m     \u001b[39mfor\u001b[39;00m tp, records \u001b[39min\u001b[39;00m six\u001b[39m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1118\u001b[0m         \u001b[39m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m         \u001b[39m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m         \u001b[39m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m         \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m records:\n\u001b[0;32m   1122\u001b[0m             \u001b[39m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m             \u001b[39m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m             \u001b[39m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m             \u001b[39m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m remaining \u001b[39m=\u001b[39m timeout_ms\n\u001b[0;32m    654\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     records \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_once(remaining, max_records, update_offsets\u001b[39m=\u001b[39;49mupdate_offsets)\n\u001b[0;32m    656\u001b[0m     \u001b[39mif\u001b[39;00m records:\n\u001b[0;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m records\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mpoll(timeout_ms\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    701\u001b[0m timeout_ms \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout_ms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mtime_to_next_poll() \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[1;32m--> 702\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mpoll(timeout_ms\u001b[39m=\u001b[39;49mtimeout_ms)\n\u001b[0;32m    703\u001b[0m \u001b[39m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coordinator\u001b[39m.\u001b[39mneed_rejoin():\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    599\u001b[0m             timeout \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(timeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mretry_backoff_ms\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    600\u001b[0m         timeout \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, timeout)  \u001b[39m# avoid negative timeouts\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[0;32m    604\u001b[0m \u001b[39m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[39m# if handlers need to acquire locks\u001b[39;00m\n\u001b[0;32m    606\u001b[0m responses\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\kafka\\client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_send_sockets()\n\u001b[0;32m    633\u001b[0m start_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 634\u001b[0m ready \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[0;32m    635\u001b[0m end_select \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sensors:\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\selectors.py:324\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     r, w, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_readers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_writers, [], timeout)\n\u001b[0;32m    325\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32mc:\\Users\\AhmaDGoly\\AppData\\Local\\Programs\\Python\\Python39\\lib\\selectors.py:315\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_select\u001b[39m(\u001b[39mself\u001b[39m, r, w, _, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 315\u001b[0m     r, w, x \u001b[39m=\u001b[39m select\u001b[39m.\u001b[39;49mselect(r, w, w, timeout)\n\u001b[0;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m r, w \u001b[39m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for message in consumer:\n",
    "    x = x + 1\n",
    "    print(f\"{current_time()}: {x}: Received a news from Preprocessing_Thread. putting it to cassandra...\")\n",
    "    json_row = message.value.decode('utf-8')\n",
    "    row = json.loads(json_row)\n",
    "    #receivedRowDF = pd.DataFrame([row])\n",
    "\n",
    "    #az inja be bad kodetoon ro be ezaye har satre jadidi ke omad piade konin.\n",
    "    MsgQueue.put(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfe9f1fea70b6f3c892a507b25933db96548476c663a42d0d97c2442f6e3001f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
