{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtuE4Ql7RaL_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "osdb_PfaRhfN",
        "outputId": "bf2fe68f-3753-41e6-e613-d8a054eadbe7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.9.1-py3-none-any.whl (349 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/349.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/349.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.7/349.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Collecting numpy<2.0.0,>=1.24.3 (from hazm)\n",
            "  Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.65.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.1.0)\n",
            "Installing collected packages: python-crfsuite, numpy, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hazm-0.9.1 numpy-1.25.0 python-crfsuite-0.9.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the file URLs\n",
        "file_urls = [\n",
        "    \"https://drive.google.com/file/d/1xz_YBI8B4fHqAmxlb_3JE5ecJZS5V5z8/view?usp=sharing\",\n",
        "    \"https://drive.google.com/file/d/1z9nYQM2wYVQ7zChtShR6ZaHBSxst_UkK/view?usp=sharing\"\n",
        "]\n",
        "\n",
        "# Download files\n",
        "for url in file_urls:\n",
        "    file_id = url.split(\"/\")[5]\n",
        "    output_file = f\"file_{file_id}.zip\"  # Customize the output file name if needed\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file, quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNUKC_6aRiYv",
        "outputId": "b16515c1-1211-47ba-eca1-a1053423360f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xz_YBI8B4fHqAmxlb_3JE5ecJZS5V5z8\n",
            "To: /content/file_1xz_YBI8B4fHqAmxlb_3JE5ecJZS5V5z8.zip\n",
            "100%|██████████| 271M/271M [00:01<00:00, 229MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z9nYQM2wYVQ7zChtShR6ZaHBSxst_UkK\n",
            "To: /content/file_1z9nYQM2wYVQ7zChtShR6ZaHBSxst_UkK.zip\n",
            "100%|██████████| 18.3M/18.3M [00:00<00:00, 207MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.rename('/content/file_1z9nYQM2wYVQ7zChtShR6ZaHBSxst_UkK.zip', '/content/test_cleaned.csv')\n",
        "os.rename('/content/file_1xz_YBI8B4fHqAmxlb_3JE5ecJZS5V5z8.zip', '/content/train_cleaned.csv')\n"
      ],
      "metadata": {
        "id": "GSi3o4XORjkN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from hazm import *\n",
        "# Load the data\n",
        "train_df = pd.read_csv('train_cleaned.csv')\n",
        "test_df = pd.read_csv('test_cleaned.csv')\n"
      ],
      "metadata": {
        "id": "Qp94ZJhQSc-D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PREPROCESSING\n",
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "train_df = train_df.replace('\\[n\\]', '', regex=True)\n",
        "test_df = test_df.replace('\\[n\\]', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace('«', ' ', regex=True)\n",
        "test_df = test_df.replace('«', ' ', regex=True)\n",
        "\n",
        "train_df = train_df.replace('»', ' ', regex=True)\n",
        "test_df = test_df.replace('»', ' ', regex=True)\n",
        "\n",
        "train_df = train_df.replace('\\(', ' ', regex=True)\n",
        "test_df = test_df.replace('\\(', ' ', regex=True)\n",
        "\n",
        "train_df = train_df.replace('\\)', ' ', regex=True)\n",
        "test_df = test_df.replace('\\)', ' ', regex=True)\n",
        "\n",
        "train_df = train_df.replace(':', ' ', regex=True)\n",
        "test_df = test_df.replace(':', ' ', regex=True)\n",
        "\n",
        "train_df = train_df.replace('  ', ' ', regex=True)\n",
        "test_df = test_df.replace('  ', ' ', regex=True)\n",
        "\n",
        "train_df = train_df.replace('  ', ' ', regex=True)\n",
        "test_df = test_df.replace('  ', ' ', regex=True)"
      ],
      "metadata": {
        "id": "qjCTRfcjSZAk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in train_df.iterrows():\n",
        "    article = row['article']\n",
        "    start_index = article.find(\"به گزارش\")\n",
        "    end_index = article.find(\"،\", start_index)\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        article = article[:start_index] + article[end_index+1:]\n",
        "    train_df.at[index, 'article'] = article\n",
        "for index, row in test_df.iterrows():\n",
        "    article = row['article']\n",
        "    start_index = article.find(\"به گزارش\")\n",
        "    end_index = article.find(\"،\", start_index)\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        article = article[:start_index] + article[end_index+1:]\n",
        "    test_df.at[index, 'article'] = article"
      ],
      "metadata": {
        "id": "1nqBY2ZlSZoa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.replace(' به ', '', regex=True)\n",
        "test_df = test_df.replace(' به ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' با ', '', regex=True)\n",
        "test_df = test_df.replace(' با ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' در ', '', regex=True)\n",
        "test_df = test_df.replace(' در ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' برای ', '', regex=True)\n",
        "test_df = test_df.replace(' برای ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' از ', '', regex=True)\n",
        "test_df = test_df.replace(' از ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' بر ', '', regex=True)\n",
        "test_df = test_df.replace(' بر ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' که ', '', regex=True)\n",
        "test_df = test_df.replace(' که ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' را ', '', regex=True)\n",
        "test_df = test_df.replace(' را ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' او ', '', regex=True)\n",
        "test_df = test_df.replace(' او ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' آن ', '', regex=True)\n",
        "test_df = test_df.replace(' آن ', '', regex=True)\n",
        "\n",
        "train_df = train_df.replace(' آن ', '', regex=True)\n",
        "test_df = test_df.replace(' آن ', '', regex=True)"
      ],
      "metadata": {
        "id": "hzsyLvt1S3Xq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.replace('  ', ' ', regex=True)\n",
        "test_df = test_df.replace('  ', ' ', regex=True)\n",
        "\n",
        "train_df = train_df.replace('  ', ' ', regex=True)\n",
        "test_df = test_df.replace('  ', ' ', regex=True)"
      ],
      "metadata": {
        "id": "fvMNGOq7S9E6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "stopwords = stopwords_list()"
      ],
      "metadata": {
        "id": "mOo1b6IlSu37"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekvjZwnYTtaL",
        "outputId": "c9beff66-7d16-44eb-bed6-78b57676a03f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['آخرین', 'آقای', 'آمد', 'آمده', 'آمده_است', 'آن', 'آنان', 'آنجا', 'آنها', 'آنچه', 'آنکه', 'آورد', 'آوری', 'آیا', 'ابتدا', 'اثر', 'اجرا', 'اخیر', 'از', 'است', 'اش', 'اغلب', 'افراد', 'افرادی', 'افزود', 'البته', 'اما', 'امر', 'امکان', 'اند', 'او', 'اول', 'اولین', 'اکنون', 'اگر', 'ایشان', 'این', 'اینجا', 'اینکه', 'با', 'بار', 'باره', 'باز', 'باشد', 'باشند', 'باعث', 'بالا', 'باید', 'بخش', 'بخشی', 'بدون', 'بر', 'برابر', 'براساس', 'برای', 'برخی', 'برداری', 'بروز', 'بزرگ', 'بسیار', 'بسیاری', 'بعد', 'بعضی', 'بلکه', 'بنابراین', 'بندی', 'به', 'بهتر', 'بهترین', 'بود', 'بودن', 'بودند', 'بوده', 'بوده_است', 'بی', 'بیان', 'بیرون', 'بیش', 'بیشتر', 'بیشتری', 'بین', 'تا', 'تاکنون', 'تبدیل', 'تحت', 'ترتیب', 'تعداد', 'تعیین', 'تغییر', 'تمام', 'تمامی', 'تنها', 'تهیه', 'تو', 'جا', 'جاری', 'جای', 'جایی', 'جدی', 'جدید', 'جریان', 'جز', 'جمع', 'جمعی', 'حال', 'حالا', 'حالی', 'حتی', 'حد', 'حداقل', 'حدود', 'حل', 'خاص', 'خاطرنشان', 'خصوص', 'خطر', 'خواهد_بود', 'خواهد_شد', 'خواهد_کرد', 'خوب', 'خوبی', 'خود', 'خودش', 'خویش', 'خیلی', 'داد', 'دادن', 'دادند', 'داده', 'داده_است', 'دار', 'دارای', 'دارد', 'دارند', 'داریم', 'داشت', 'داشتن', 'داشتند', 'داشته', 'داشته_است', 'داشته_باشد', 'داشته_باشند', 'دانست', 'در', 'درباره', 'درون', 'دسته', 'دهد', 'دهند', 'دهه', 'دو', 'دوباره', 'دور', 'دوم', 'دچار', 'دیگر', 'دیگران', 'دیگری', 'را', 'راه', 'رسید', 'رسیدن', 'رشد', 'رفت', 'رو', 'روبه', 'روش', 'روند', 'روی', 'ریزی', 'زاده', 'زیاد', 'زیادی', 'زیر', 'زیرا', 'ساز', 'سازی', 'ساله', 'سالهای', 'سال\\u200cهای', 'سایر', 'سبب', 'سراسر', 'سعی', 'سمت', 'سه', 'سهم', 'سوم', 'سوی', 'سپس', 'سی', 'شامل', 'شان', 'شاید', 'شخصی', 'شد', 'شدن', 'شدند', 'شده', 'شده_است', 'شده_اند', 'شده_بود', 'شروع', 'شش', 'شما', 'شمار', 'شود', 'شوند', 'صرف', 'ضمن', 'طبق', 'طرف', 'طور', 'طول', 'طی', 'ع', 'عالی', 'عدم', 'علاوه', 'علت', 'علیه', 'عهده', 'عین', 'غیر', 'فرد', 'فردی', 'فقط', 'فوق', 'فکر', 'قابل', 'قبل', 'لازم', 'لحاظ', 'لذا', 'ما', 'مانند', 'متاسفانه', 'متر', 'متفاوت', 'مثل', 'محسوب', 'مدت', 'مربوط', 'مشخص', 'ممکن', 'من', 'مناسب', 'منظور', 'مهم', 'مواجه', 'موجب', 'مورد', 'می', 'میان', 'می\\u200cآید', 'می\\u200cباشد', 'می\\u200cتوان', 'می\\u200cتواند', 'می\\u200cتوانند', 'می\\u200cدهد', 'می\\u200cدهند', 'می\\u200cرسد', 'می\\u200cرود', 'می\\u200cشد', 'می\\u200cشود', 'می\\u200cشوند', 'می\\u200cکرد', 'می\\u200cکردند', 'می\\u200cکند', 'می\\u200cکنم', 'می\\u200cکنند', 'می\\u200cکنیم', 'می\\u200cگوید', 'می\\u200cگویند', 'می\\u200cگیرد', 'می\\u200cیابد', 'ناشی', 'نباید', 'نبود', 'نحوه', 'نخست', 'نخستین', 'ندارد', 'ندارند', 'نزدیک', 'نسبت', 'نشست', 'نظر', 'نظیر', 'نمی\\u200cشود', 'نه', 'نوع', 'نوعی', 'نیاز', 'نیز', 'نیست', 'نیستند', 'نیمه', 'هایی', 'هر', 'هستند', 'هستیم', 'هم', 'همان', 'همه', 'همواره', 'همچنان', 'همچنین', 'همچون', 'همیشه', 'همین', 'هنوز', 'هنگام', 'هیچ', 'و', 'وارد', 'وجود', 'وقتی', 'ولی', 'وگو', 'وی', 'ویژه', 'پخش', 'پر', 'پس', 'پشت', 'پنج', 'پی', 'پیدا', 'پیش', 'چرا', 'چند', 'چنین', 'چه', 'چهار', 'چهارم', 'چون', 'چگونه', 'چیز', 'چیزی', 'کافی', 'کامل', 'کاملا', 'کدام', 'کرد', 'کردم', 'کردن', 'کردند', 'کرده', 'کرده_است', 'کرده_اند', 'کرده_بود', 'کسانی', 'کسی', 'کل', 'کلی', 'کم', 'کمی', 'کنار', 'کند', 'کنم', 'کنند', 'کننده', 'کنندگان', 'کنید', 'کنیم', 'که', 'کوچک', 'گاه', 'گذاری', 'گردد', 'گرفت', 'گرفته', 'گرفته_است', 'گروهی', 'گفت', 'گفته', 'گونه', 'گیرد', 'گیری', 'یا', 'یابد', 'یافت', 'یافته', 'یافته_است', 'یعنی', 'یک', 'یکدیگر', 'یکی']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['summary'] = train_df['summary'].apply(normalizer.normalize)\n",
        "train_df['article'] = train_df['article'].apply(normalizer.normalize)\n",
        "test_df['summary'] = test_df['summary'].apply(normalizer.normalize)\n",
        "test_df['article'] = test_df['article'].apply(normalizer.normalize)"
      ],
      "metadata": {
        "id": "JXUJuJquTrx6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['summary'] = train_df['summary'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords))\n",
        "train_df['article'] = train_df['article'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords))\n",
        "test_df['summary'] = test_df['summary'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords))\n",
        "test_df['article'] = test_df['article'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords))"
      ],
      "metadata": {
        "id": "HDcVKEDkULeM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFC8xxwPVPWR",
        "outputId": "8f1737d3-64d0-4d32-dae8-b51cfdb0eae1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 article  \\\n",
            "0      علی کاردر امروز ۲۷ دی ماهمراسم تودیع محسن قمصر...   \n",
            "1      علی‌اصغر گودرزی‌فراهانیاشارهاینکه طرح‌هایحال ا...   \n",
            "2      جمعیکارشناسان پالایشگاههدف معرفی محصول کود گوگ...   \n",
            "3      سعید نظریصفحه اینستاگرام نوشتحالیکه شهر شیراز ...   \n",
            "4      سیدباقر مرتضوی، مشاور وزیر نفت مدیرکل اچ اس یی...   \n",
            "...                                                  ...   \n",
            "82017  تیم‌های ملی هاکی زنان مردان ایرانسومین دیدار خ...   \n",
            "82018  مصطفی قلی‌خسروی افزودکشورهای رقم درصداست کهفرو...   \n",
            "82019  نقلپایگاه اطلاع‌رسانی کمیته امداد، سیدمرتضی بخ...   \n",
            "82020  طرحامروز پنجشنبه بیستم ساعت ۱۴ روز یکشنبه ۲۳ ش...   \n",
            "82021  مجموع کاهش تولید موثر کشورهای عضو غیرعضو اوپک ...   \n",
            "\n",
            "                                                 summary  \n",
            "0      مدیرعامل شرکت ملی نفت، عملکرد مدیریت امور بین‎...  \n",
            "1      سرپرست مدیریت برنامه‌ریزی توسعه شرکت ملی صنایع...  \n",
            "2      پالایشگاه گاز خانگیرانهدف معرفی گوگرد بنتونیتی...  \n",
            "3      سخنگوی شورای شهر شیراز عمرانی شهرسازیاین شهرفر...  \n",
            "4      مشاور وزیر نفت مدیرکل اچ اس یی پدافند غیرعامل ...  \n",
            "...                                                  ...  \n",
            "82017  تیم‌های ملی هاکی زنان مردان ایرانسومین دیدار خ...  \n",
            "82018  قلی‌خسروی، رئیس اتحادیه مشاوران املاک تهران نر...  \n",
            "82019  رئیس کمیته امدادآغاز مرحله پویش ایران همدل همز...  \n",
            "82020  گروه صنعتی ایران‌خودرومنظور تامین مشتریان، سوم...  \n",
            "82021  مجموع کاهش جهانی تولید نفت خام می‌تواندبیشروزا...  \n",
            "\n",
            "[82013 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_articles = train_df['article'].apply(lambda x: x.split())\n",
        "tokenized_summaries = train_df['summary'].apply(lambda x: x.split())\n",
        "\n",
        "from hazm import Lemmatizer\n",
        "lemmatizer = Lemmatizer()\n",
        "lemmatized_articles = tokenized_articles.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
        "lemmatized_summaries = tokenized_summaries.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
        "sentences = lemmatized_articles + lemmatized_summaries\n",
        "word2vec_model = Word2Vec(sentences, vector_size=200, window=5, min_count=50, workers=4)\n"
      ],
      "metadata": {
        "id": "_0wn0P8zUd20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del sentences\n",
        "\n",
        "word_vectors = word2vec_model.wv\n",
        "vocab_size = len(word_vectors.key_to_index)\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "\n",
        "input_sequences = [[word_vectors.key_to_index[word] for word in article if word in word_vectors.key_to_index] for article in lemmatized_articles]\n",
        "target_sequences = [[word_vectors.key_to_index[word] for word in summary if word in word_vectors.key_to_index] for summary in lemmatized_summaries]"
      ],
      "metadata": {
        "id": "PZaMEVUbdZRH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec_model TEST:\n",
        "vocab_size = len(word2vec_model.wv.key_to_index)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "print(\"Embedding dimension:\", embedding_dim)\n",
        "word = \"اکبر\"\n",
        "word_vector = word2vec_model.wv[word]\n",
        "print(\"Vector representation of\", word, \":\\n\", word_vector)\n",
        "\n",
        "similar_words = word2vec_model.wv.most_similar(word)\n",
        "print(\"Similar words to\", word, \":\\n\", similar_words)\n",
        "\n",
        "similar_words = word2vec_model.wv.most_similar('پردازنده')\n",
        "print(\"Similar words to پردازنده :\\n\", similar_words)\n",
        "#END OF word2vec_model TEST:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4s1F8j7YH-e",
        "outputId": "fc997637-7d0c-41e6-bea6-caa5afef484d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 18173\n",
            "Embedding dimension: 200\n",
            "Vector representation of اکبر :\n",
            " [ 1.2350199   0.280907   -0.40399185 -0.20215182 -0.5943895   0.45831972\n",
            " -1.0339925   0.55347234 -0.44394156  1.6679451  -0.49446937  0.07471522\n",
            " -0.12149055  0.4831254   0.06544478  1.3032215  -1.3123544  -0.7039888\n",
            "  0.44597727 -0.7884696   0.05496774 -0.5730534  -0.82563394  0.0495027\n",
            " -0.68077105  0.19453597  0.79829437 -0.673367   -1.4534003  -0.03857362\n",
            "  1.262686    0.54629976 -0.66839254  0.20944403  0.84818304 -0.15494092\n",
            "  0.6310205  -0.18958978  0.24378341  0.66389865 -0.87286013 -1.1098772\n",
            "  0.12551327 -0.12275736  0.33487585 -0.14404838  0.84873587 -0.14195542\n",
            " -0.07694794 -0.08701669  1.0913901   1.0149726  -0.0270554  -0.13456213\n",
            "  0.6566119   0.2969537   0.1117627   0.03661583 -0.5945651   0.17060229\n",
            " -0.8888519   0.41917536 -0.18336241  1.2918079  -0.27415344  0.14106542\n",
            " -0.58855456  1.2981962  -0.19965047  0.63436294  0.16668017 -1.254201\n",
            "  0.09732016 -0.6503214  -0.12611039  0.84011227  0.1913886  -0.8379491\n",
            "  0.06988283  0.18349707  0.23742871 -0.18347201  0.02381667  1.0217904\n",
            " -0.73485297 -0.06229011 -0.9159981  -0.11130853  0.6388987  -0.21305363\n",
            "  0.3824724   0.04402085  0.05640685 -0.05165069 -0.37477088 -0.0375987\n",
            "  0.3537449  -0.02449278  0.02282355  0.7050877  -0.45567366 -0.01894157\n",
            "  0.69604653 -0.22358374 -1.3669155   0.22182478  0.16623002  0.7024239\n",
            "  0.16206476  0.53005296 -0.3099821   0.5944934   0.19627984 -0.27267128\n",
            "  0.9748681  -0.8585596  -0.10176902  0.298726    0.15651369 -0.155532\n",
            "  0.272857    0.7106548   0.85586077 -0.3271607   0.6040998   1.5215603\n",
            " -0.6328504  -0.35532048 -0.13825864  0.6149949  -0.58500755  0.69346106\n",
            " -0.8932219   0.34454417  0.29430604  0.25161573  1.3150874   0.50345516\n",
            "  0.86502844 -0.64524543 -0.7167685  -0.05350032 -0.5199856   0.56355965\n",
            " -0.8068864  -0.0888413  -0.40458834  0.8795362   0.15338646 -0.0343896\n",
            " -0.40709487 -1.2549835   0.5667102  -0.92527145  0.69217587  0.35779202\n",
            "  0.04772245 -0.23479004 -0.29840162 -0.13757338  0.49711886 -0.09642667\n",
            " -0.12141202  0.8137569  -0.36651886  0.14407615  1.1501552  -0.66750956\n",
            " -0.18783277  0.65775913 -1.735593   -0.7574023   0.00891716  0.38092643\n",
            "  0.34781605  0.34744683 -0.02862081  0.63192415  0.11814602  0.9407622\n",
            "  0.7931648   0.31080806 -0.22187874  0.06966525  0.3655417  -0.04695658\n",
            " -1.0002005  -0.541514   -0.0824779   0.75365657  0.48500705  0.1080052\n",
            "  0.67106825 -1.3580927   0.45318374 -0.08866972  1.184529   -0.02182812\n",
            "  0.34700835 -0.79492056]\n",
            "Similar words to اکبر :\n",
            " [('شهباز', 0.7874920964241028), ('سیروس', 0.7735888957977295), ('محمودی،', 0.7715791463851929), ('وحید', 0.7703215479850769), ('افشین', 0.7654445767402649), ('حاج', 0.7651498913764954), ('اسماعیل', 0.7569391131401062), ('افشار', 0.7561052441596985), ('ابوالفضل', 0.7540026903152466), ('سیدمحمد', 0.7538558840751648)]\n",
            "Similar words to پردازنده :\n",
            " [('Core', 0.8475980162620544), ('i', 0.8437730073928833), ('اینتل', 0.8386759757995605), ('X', 0.8361746072769165), ('نمایشگر', 0.8340058326721191), ('Huawei', 0.8339452147483826), ('لپ\\u200cتاپ', 0.8323522210121155), ('Galaxy', 0.8290748000144958), ('تراشه', 0.8283674716949463), ('پیکسل', 0.8227652907371521)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_lengths = lemmatized_articles.apply(lambda x: len(x))\n",
        "summary_lengths = lemmatized_summaries.apply(lambda x: len(x))\n",
        "\n",
        "print(\"Article length statistics:\")\n",
        "print(article_lengths.describe())\n",
        "\n",
        "print(\"\\nSummary length statistics:\")\n",
        "print(summary_lengths.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv2Ium--YONY",
        "outputId": "82c5b9e3-c9e2-45ff-dc8d-dfe461dd01b8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article length statistics:\n",
            "count    82013.000000\n",
            "mean       176.135027\n",
            "std        122.827211\n",
            "min          2.000000\n",
            "25%        102.000000\n",
            "50%        140.000000\n",
            "75%        205.000000\n",
            "max       1837.000000\n",
            "Name: article, dtype: float64\n",
            "\n",
            "Summary length statistics:\n",
            "count    82013.000000\n",
            "mean        17.949313\n",
            "std          5.964220\n",
            "min          4.000000\n",
            "25%         14.000000\n",
            "50%         17.000000\n",
            "75%         21.000000\n",
            "max         89.000000\n",
            "Name: summary, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_input = 250\n",
        "max_length_output = 25\n",
        "padded_inputs = pad_sequences(input_sequences, maxlen=max_length_input, padding='post')\n",
        "padded_targets = pad_sequences(target_sequences, maxlen=max_length_output, padding='post')"
      ],
      "metadata": {
        "id": "wdt-aDPvYW6g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = 0.15\n",
        "validation_df, test_df = train_test_split(test_df, test_size=1-validation_size, random_state=42)\n",
        "print(f\"Validation set size: {len(validation_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vhgEyE3WZdx",
        "outputId": "401acc42-7d74-4c50-f77c-3b0a45fe2ca3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set size: 838\n",
            "Test set size: 4755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test Data:\n",
        "tokenized_articles_test = test_df['article'].apply(lambda x: x.split())\n",
        "tokenized_summaries_test = test_df['summary'].apply(lambda x: x.split())\n",
        "\n",
        "lemmatized_articles_test = tokenized_articles_test.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
        "lemmatized_summaries_test = tokenized_summaries_test.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
        "\n",
        "input_sequences_test = [[word_vectors.key_to_index[word] for word in article if word in word_vectors.key_to_index] for article in lemmatized_articles_test]\n",
        "target_sequences_test = [[word_vectors.key_to_index[word] for word in summary if word in word_vectors.key_to_index] for summary in lemmatized_summaries_test]\n",
        "\n",
        "padded_inputs_test = pad_sequences(input_sequences_test, maxlen=max_length_input, padding='post')\n",
        "padded_targets_test = pad_sequences(target_sequences_test, maxlen=max_length_output, padding='post')\n",
        "\n",
        "#Valid Data:\n",
        "tokenized_articles_valid = validation_df['article'].apply(lambda x: x.split())\n",
        "tokenized_summaries_valid = validation_df['summary'].apply(lambda x: x.split())\n",
        "\n",
        "lemmatized_articles_valid = tokenized_articles_valid.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
        "lemmatized_summaries_valid = tokenized_summaries_valid.apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
        "\n",
        "input_sequences_valid = [[word_vectors.key_to_index[word] for word in article if word in word_vectors.key_to_index] for article in lemmatized_articles_valid]\n",
        "target_sequences_valid = [[word_vectors.key_to_index[word] for word in summary if word in word_vectors.key_to_index] for summary in lemmatized_summaries_valid]\n",
        "\n",
        "padded_inputs_valid = pad_sequences(input_sequences_valid, maxlen=max_length_input, padding='post')\n",
        "padded_targets_valid = pad_sequences(target_sequences_valid, maxlen=max_length_output, padding='post')"
      ],
      "metadata": {
        "id": "7NBMQYLAXPLw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.save(\"/content/drive/MyDrive/FinalFilesForPD/word2vec_model.bin\")\n",
        "\n",
        "import pickle\n",
        "variables = {'input1': padded_inputs, 'output1': padded_targets, 'input2': padded_inputs_test,'output2': padded_targets_test, 'input3': padded_inputs_valid, 'output3': padded_targets_valid , 'info':[250,25,82013,4755,838]}"
      ],
      "metadata": {
        "id": "BIsC9hyfXWVp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/FinalFilesForPD/allInputs.pickle', 'wb') as f:\n",
        "    pickle.dump(variables, f)"
      ],
      "metadata": {
        "id": "vVqwZewGeOXt"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}